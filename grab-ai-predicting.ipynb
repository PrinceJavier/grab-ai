{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Predicting\n",
    "by Prince Joseph Erneszer Javier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on Virtual Machine and Environment**\n",
    "<br>Virtual Machine: AWS c5.9xlarge\n",
    "<br>Operating System: Deep Learning AMI (Ubuntu) Version 23.0\n",
    "<br>Environment: tensorflow_p36\n",
    "<br>Storage Size: 85 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use trained machine learning classifiers to predict on new data if driving is safe or unsafe. This is part of my entry for Grab AI for SEA challenge under the Safety category. We were provided Telematics data and from these data we would develop models that can predict if a driver is driving safely or not. The raw features dataset contains 16 million samples and 11 columns including the `bookingID`. There are 20,000 unique `bookingID`'s each with either 0 or 1 corresponding to safe or unsafe driving. The data were preprocessed in `grab-ai-preprocessing-eda`. The models were trained in `grab-ai-training`. In this notebook, we developed a pipeline for predicting on a new dataset. The predictions are then saved as a CSV under the folder `prediction`. The CSV contains two columns: `bookingID` and the predicted class (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab AI for SEA challenge is a hackathon organized by Grab. Grab offers three challenges that can be solved using AI: Traffic Management, Computer Vision, and Safety. We tackle the Safety Challenge. The `Ride Safety` dataset was provided by Grab, which contains Telematics data (acceleration, gyroscope data, speed, etc.), `bookingID`, and labels (0 or 1 for safe or unsafe driving). The raw dataset was prepared in `grab-ai-preprocessing-eda`. The output of that notebook is used as input for machine learning classifier training. This notebook contains the pipeline for predicting on new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Ride Safety` dataset contains two folders: `features` and `labels`. `features` contains 10 CSV files which contain a total of 16 million telematics data samples. The columns in the `features` dataset as described in `data_dictionary.xlsx` are:\n",
    "\n",
    "|Column Name|Description|\n",
    "|:--|:--|\n",
    "|`bookingID`|trip id|\n",
    "|`Accuracy`|accuracy inferred by GPS in meters|\n",
    "|`Bearing`|GPS bearing|\n",
    "|`acceleration_x`|accelerometer reading in x axis (m/s2)|\n",
    "|`acceleration_y`|accelerometer reading in y axis (m/s2)|\n",
    "|`acceleration_z`|accelerometer reading in z axis (m/s2)|\n",
    "|`gyro_x`|gyroscope reading in x axis (rad/s)|\n",
    "|`gyro_y`|gyroscope reading in y axis (rad/s)|\n",
    "|`gyro_z`|gyroscope reading in z axis (rad/s)|\n",
    "|`second`|time of the record by number of seconds|\n",
    "|`Speed`|speed measured by GPS in m/s|\n",
    "\n",
    "In `grab-ai-preprocessing-eda`, the samples were aggregated and features were engineered. `bookingID` and `second` were not included as features. The following measures were calculated for each feature: min, max, range, mean, standard deviation, skewness, kurtosis, dominant frequency (from fourier transform periodogram), and maximum power (from fourier transform periodogram). An additional feature was added which is the trip length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features dataset should be in CSV format. The CSV files should follow the same format as the CSV files in the `features` folder in the training set. The columns are the 11 columns in the table above. Save all features CSVs under one folder. The folder's default path is `data/for_prediction/` and the default filepath for the predictions is `prediction/predictions.csv`, but you can change them as needed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths of features\n",
    "path_feats =\"data/for_prediction/\"\n",
    "\n",
    "# path of predictions csv file\n",
    "filepath = \"prediction/predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the CSVs are placed in the paths above, just restart and run the entire Jupyter notebook. The predicted classes per `bookingID` will be saved as CSV in the folder `prediction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import periodogram\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contents of dataset folder\n",
    "paths = glob.glob(path_feats+'*')\n",
    "\n",
    "# let's combine all feature into one pandas dataframe\n",
    "df_feats = pd.DataFrame()\n",
    "\n",
    "for path in paths:\n",
    "    _ = pd.read_csv(path, header=\"infer\")\n",
    "    df_feats = pd.concat([df_feats, _])\n",
    "    \n",
    "df_feats = df_feats.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_f(y):\n",
    "    \"\"\"Given time series y, get frequency of maximum power\n",
    "    from periodogram\"\"\"\n",
    "    f, p = periodogram(y, scaling='spectrum')\n",
    "    ind = np.argsort(p)\n",
    "    f_max = f[ind[-1]]\n",
    "    return f_max\n",
    "\n",
    "def max_power(y):\n",
    "    \"\"\"Given time series y, get maximum power\"\"\"\n",
    "    f, p = periodogram(y, scaling='spectrum')\n",
    "    return p.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2223: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# we engineer feature, aggregating feature values per bookingID\n",
    "# getting min, max, range, mean, std, skewness, and kurtosis\n",
    "\n",
    "df_engg_feats = df_feats.drop(\"second\", axis=1)\n",
    "df_engg_feats = df_engg_feats.groupby(by=\"bookingID\", as_index=True).agg([np.min, np.max, np.ptp, np.mean, np.std, skew, kurtosis, dominant_f, max_power])\n",
    "\n",
    "# flatten column names\n",
    "cols = [df_engg_feats.columns[i][0]+\"_\"+df_engg_feats.columns[i][1] for i in range(len(df_engg_feats.columns))]\n",
    "\n",
    "df_engg_feats.columns = cols\n",
    "df_engg_feats.reset_index(inplace=True)\n",
    "\n",
    "# add length of each trip\n",
    "df_len = df_feats.groupby(by=\"bookingID\", as_index=True).agg(len).iloc[:, 0:1]\n",
    "df_len.columns = ['trip_len']\n",
    "df_len.reset_index(inplace=True)\n",
    "\n",
    "# merge along bookingID\n",
    "df_engg_feats_2 = pd.merge(df_engg_feats, df_len, how=\"inner\", on=\"bookingID\")\n",
    "\n",
    "# get booking ID\n",
    "bookingID = df_engg_feats_2.bookingID\n",
    "\n",
    "# get feature set\n",
    "X0 = df_engg_feats_2.drop([\"bookingID\"], axis=1)\n",
    "cols = X0.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble of top models\n",
    "\n",
    "def predict(scaler=\"minmax\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given scaler name, scale dataset and predict using pretrained models\n",
    "    Uses all features\n",
    "    Return prediction as array of 0s and 1s from all models\n",
    "    \"\"\"\n",
    "\n",
    "    # scaling\n",
    "    path = f\"scalers/{scaler}.sav\"\n",
    "    sc = pickle.load(open(path, 'rb'))\n",
    "    \n",
    "    y_preds = []\n",
    "\n",
    "    # scale X\n",
    "    X = pd.DataFrame(sc.transform(X0))\n",
    "    X.columns = cols\n",
    "\n",
    "    model_names = [f\"models/{model}_{scaler}.sav\" for model in ['gbm', 'svc']]\n",
    "\n",
    "    for model_name in model_names:\n",
    "        # load the model from disk (machine learning)\n",
    "        filename = model_name\n",
    "        model = pickle.load(open(filename, 'rb'))\n",
    "        y_pred = model.predict(X)\n",
    "        y_preds.append(y_pred)\n",
    "    \n",
    "    return y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_minmax = predict(\"minmax\")\n",
    "y_pred_std = predict(\"std\")\n",
    "\n",
    "# concatenate results from std and minmax models\n",
    "y_pred_all = y_pred_std + y_pred_minmax\n",
    "\n",
    "# # get average of all values\n",
    "y_pred = np.round(np.array([np.array(i).flatten() for i in y_pred_all]).mean(axis=0), 0)\n",
    "\n",
    "# append bookingID with y_pred\n",
    "predictions = pd.DataFrame(bookingID)\n",
    "predictions['label'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookingID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>91</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bookingID  label\n",
       "0          8    0.0\n",
       "1         13    1.0\n",
       "2         33    1.0\n",
       "3         35    1.0\n",
       "4         91    0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save predictions to CSV\n",
    "predictions.to_csv(filepath, index=False)\n",
    "predictions.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
